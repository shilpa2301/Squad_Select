{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXcO2n-EcUPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ostLPbDFULzN"
      },
      "source": [
        "# **READ ABOUT:**\n",
        "\n",
        "Implicit group recommendation techniques paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJX_5E7fXsN2"
      },
      "source": [
        "**For Implicit Rating**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDGHlDKYTNzu"
      },
      "outputs": [],
      "source": [
        "#using jaccard - own algorithm\n",
        "import itertools\n",
        "def implicit_dissimilarity_jaccard_func(implicit_user_trainMatrix):\n",
        "\n",
        "    \n",
        "    Dissimilarity_mat_implicit=np.zeros((num_groups, num_items))\n",
        "\n",
        "    for group_id in dataset.group_user_Dict:\n",
        "        #print(group_id)\n",
        "        group_members= dataset.group_user_Dict[group_id]\n",
        "        num_members=len(group_members)\n",
        "        for i in range(num_items):\n",
        "            dissimilarity=0\n",
        "            for m, n in itertools.combinations(range(num_members), 2):\n",
        "\n",
        "              user1_items = implicit_user_trainMatrix[m]\n",
        "              user2_items = implicit_user_trainMatrix[n]\n",
        "              jaccard_similarity = np.sum(np.logical_and(user1_items, user2_items)) / np.sum(np.logical_or(user1_items, user2_items))\n",
        "\n",
        "              dissimilarity = dissimilarity+ (1 - jaccard_similarity)\n",
        "            Dissimilarity_mat_implicit[group_id][i]=(2*dissimilarity)/(num_members*(num_members-1) )\n",
        "    return Dissimilarity_mat_implicit\n",
        "    #print(Dissimilarity_mat_implicit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10qkhePkWCAS"
      },
      "outputs": [],
      "source": [
        "#using pearson coeff - own algorithm\n",
        "from scipy.stats import pearsonr \n",
        "\n",
        "def implicit_dissimilarity_pearson_func(implicit_user_trainMatrix):\n",
        "\n",
        "    \n",
        "    Dissimilarity_mat_implicit_using_pearson=np.zeros((num_groups, num_items))\n",
        "\n",
        "    for group_id in dataset.group_user_Dict:\n",
        "        print(group_id)\n",
        "        group_members= dataset.group_user_Dict[group_id]\n",
        "        num_members=len(group_members)\n",
        "        for i in range(num_items):\n",
        "           dissimilarity = 0\n",
        "           for m, n in itertools.combinations(range(num_members), 2):\n",
        "               user1_items = implicit_user_trainMatrix[m]\n",
        "               user2_items = implicit_user_trainMatrix[n]\n",
        "               pearson_similarity, _ = pearsonr(user1_items, user2_items)\n",
        "               dissimilarity += 1 - pearson_similarity\n",
        "           Dissimilarity_mat_implicit_using_pearson[group_id][i]=(2*dissimilarity)/(num_members*(num_members-1) )\n",
        "    return Dissimilarity_mat_implicit_using_pearson\n",
        "    #print(Dissimilarity_mat_implicit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8ktQ_78W7B6"
      },
      "outputs": [],
      "source": [
        "Dissimilarity_mat_implicit_using_pearson=implicit_dissimilarity_pearson_func(dataset.implicit_user_trainMatrix)\n",
        "\n",
        "import pickle\n",
        "# open a file in binary mode to write the pickled data\n",
        "with open(\"Dissimilarity_mat_implicit_using_pearson.pkl\", \"wb\") as f:\n",
        "    # dump the object into the file\n",
        "    pickle.dump(Dissimilarity_mat_implicit_using_pearson, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzojAQognDwu"
      },
      "outputs": [],
      "source": [
        "#import pickle\n",
        "#with open(\"Dissimilarity_mat_implicit_using_pearson.pkl\", \"rb\") as f:\n",
        "#    # load the pickled object from the file\n",
        "#    Dissimilarity_mat_implicit_using_pearson = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9tZLmHlgT4k"
      },
      "source": [
        "**Implicit MF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJXeKFV8Uvt-"
      },
      "outputs": [],
      "source": [
        "class MF_implicit:\n",
        "\n",
        "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
        "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
        "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
        "        \n",
        "        self.latent = latent  # the latent dimension\n",
        "        self.lr = lr  # learning rate\n",
        "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
        "        \n",
        "        self.num_user, self.num_movie = train_mat.shape\n",
        "        \n",
        "        self.sample_user, self.sample_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
        "        self.num_sample = len(self.sample_user)  # the number of user-movie pairs having ratings in train_mat\n",
        "\n",
        "        self.user_test_like = []\n",
        "        for u in range(self.num_user):\n",
        "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
        "\n",
        "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
        "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
        "\n",
        "\n",
        "    def negative_sampling(self):\n",
        "        negative_movie = np.random.choice(np.arange(self.num_movie), size=(len(self.sample_user)), replace=True)\n",
        "        true_negative = self.train_mat[self.sample_user, negative_movie] == 0\n",
        "        negative_user = self.sample_user[true_negative]\n",
        "        negative_movie = negative_movie[true_negative]\n",
        "        return np.concatenate([self.sample_user, negative_user]), np.concatenate([self.sample_movie, negative_movie])\n",
        "\n",
        "    def train(self, epoch=20):\n",
        "        \"\"\"\n",
        "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
        "        Input: epoch -- the number of training epoch \n",
        "        \"\"\"\n",
        "        \n",
        "        for ep in range(epoch):\n",
        "            \"\"\" \n",
        "            Write your code here to implement the training process for one epoch, \n",
        "            at the end of each epoch, run self.test() to evaluate current version of MF.\n",
        "            \"\"\"\n",
        "            print(\"epoch={}\".format(ep))\n",
        "            negative_training_set=self.negative_sampling()\n",
        "            perm = np.random.permutation(len(negative_training_set[0]))\n",
        "\n",
        "            # use fancy indexing to select the elements in the shuffled order\n",
        "            negative_training_set_0 = negative_training_set[0][perm]\n",
        "            negative_training_set_1 = negative_training_set[1][perm]\n",
        "\n",
        "            #print(negative_training_set)\n",
        "            #print((self.P).shape)\n",
        "            #print((self.Q).shape)\n",
        "            #print((QT).shape)\n",
        "            #np.random.shuffle(negative_training_set)\n",
        "            #QT=self.Q.T\n",
        "            \n",
        "            for i in range(len(negative_training_set[0])):\n",
        "                user=negative_training_set_0[i]\n",
        "                item=negative_training_set_1[i]\n",
        "              ## Concurrently updated\n",
        "                self.P[user],self.Q[item]=self.P[user]-self.lr*(2*(self.P[user].dot(self.Q[item])-self.train_mat[user,item])*self.Q[item]\n",
        "                    +2*self.reg*self.P[user]),self.Q[item]-self.lr*(2*((self.P[user].dot(self.Q[item])-self.train_mat[user,item])*self.P[user]\n",
        "                    +2*self.reg*self.Q[item]))\n",
        "              \n",
        "            self.test()\n",
        "            \n",
        "        \"\"\"\n",
        "        End of your code for this function\n",
        "        \"\"\"\n",
        "            \n",
        "    def predict(self):\n",
        "        \"\"\"\n",
        "        Write your code here to implement the prediction function, which generates the ranked lists of movies \n",
        "        by the trained MF for every user, store the result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) \n",
        "        represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
        "        \"\"\"\n",
        "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
        "        recommendation = []\n",
        "        for u in range(self.num_user):\n",
        "            scores = prediction_mat[u]\n",
        "            train_like = np.where(train_mat[u, :] > 0)[0]\n",
        "            scores[train_like] = -9999\n",
        "            top50_iid = np.argpartition(scores, -50)[-50:]\n",
        "            top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
        "            recommendation.append(top50_iid)\n",
        "        recommendation = np.array(recommendation)\n",
        "        return recommendation\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        End of your code for this function\n",
        "        \"\"\"\n",
        "        \n",
        "    \n",
        "    def test(self):\n",
        "        recommendation = self.predict()\n",
        "\n",
        "        recalls = np.zeros(3)\n",
        "        precisions = np.zeros(3)\n",
        "        user_count = 0.\n",
        "\n",
        "        for u in range(self.num_user):\n",
        "            test_like = self.user_test_like[u]\n",
        "            test_like_num = len(test_like)\n",
        "            if test_like_num == 0:\n",
        "                continue\n",
        "            rec = recommendation[u, :]\n",
        "            hits = np.zeros(3)\n",
        "            for k in range(50):\n",
        "                if rec[k] in test_like:\n",
        "                    if k < 50:\n",
        "                        hits[2] += 1\n",
        "                        if k < 20:\n",
        "                            hits[1] += 1\n",
        "                            if k < 5:\n",
        "                                hits[0] += 1\n",
        "            recalls[0] += (hits[0] / test_like_num)\n",
        "            recalls[1] += (hits[1] / test_like_num)\n",
        "            recalls[2] += (hits[2] / test_like_num)\n",
        "            precisions[0] += (hits[0] / 5.)\n",
        "            precisions[1] += (hits[1] / 20.)\n",
        "            precisions[2] += (hits[2] / 50.)\n",
        "            user_count += 1\n",
        "\n",
        "        recalls /= user_count\n",
        "        precisions /= user_count\n",
        "\n",
        "\n",
        "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
        "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq9XAn_64mEr"
      },
      "source": [
        "**IMPLICIT RECOMMENDATION PERFORMANCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueVySoRQrH9B"
      },
      "outputs": [],
      "source": [
        "def calculate_Precision_Recall_at_K(social_relation_output_implicit, implicit_group_trainMatrix ):\n",
        "    #k=100\n",
        "  Recall_at_k=np.zeros(num_groups)\n",
        "  Precision_at_k=np.zeros(num_groups)\n",
        "  for group_id in range(len(social_relation_output_implicit)):\n",
        "      TP=0\n",
        "      TP_FP=0\n",
        "      TP_FN=np.sum(implicit_group_trainMatrix[group_id], axis=0)\n",
        "\n",
        "      for item_id in social_relation_output_implicit[group_id]:\n",
        "          if implicit_group_trainMatrix[group_id][item_id]==1:\n",
        "              TP+=1\n",
        "              TP_FP+=1\n",
        "          else:\n",
        "              TP_FP+=1\n",
        "      Precision_at_k[group_id]=(TP/TP_FP)\n",
        "      if TP_FN!=0:\n",
        "          Recall_at_k[group_id]=(TP/TP_FN)\n",
        "  return Precision_at_k, Recall_at_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glWmX-A580LD"
      },
      "source": [
        "**IMPLICIT RECOMMENDATION ANALYSIS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omNM8MTCx156"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "def Precision_at_k_plot(Precision_at_k ):\n",
        "    # corresponding x axis values\n",
        "    x = list(range(1, num_groups+1))\n",
        "\n",
        "    #fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.set_xlabel('Group Id')\n",
        "    ax.set_ylabel('Precision@100')\n",
        "    #ax.plot(x, Precision_at_k, color='blue')\n",
        "    ax.bar(x, Precision_at_k[0], color='blue') #Jaccard\n",
        "    #ax.bar(x, Precision_at_k[1], color='red') #Pearson\n",
        "    # function to show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDst-1nkyurm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "def Recall_at_k_plot(Recall_at_k ):\n",
        "    # corresponding x axis values\n",
        "    x = list(range(1, num_groups+1))\n",
        "\n",
        "    #fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax2 = fig.add_subplot(1, 1, 1)\n",
        "    ax2.set_xlabel('Group Id')\n",
        "    ax2.set_ylabel('Recall@100')\n",
        "    ax2.bar(x, Recall_at_k[0], color='blue')\n",
        "    #ax2.bar(x, Recall_at_k[1], color='red')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMq4qA05GjoT"
      },
      "source": [
        "# **IMPLICIT RECOMMENDATION ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OVuT9fT9Qzw"
      },
      "outputs": [],
      "source": [
        "#permuted pipeline creation\n",
        "#IMPLICIT\n",
        "w_1=0.8\n",
        "w_2=0.2\n",
        "\n",
        "#using Dis= Pearson/Jaccard Implicit\n",
        "overall_group_item_output_mat=concensus_heuristic_function(social_descriptor_groupwise, expertise_in_group_members_mat, Dissimilarity_mat,  dataset.implicit_user_trainMatrix, w_1, w_2)\n",
        "\n",
        "#Top k Ranking\n",
        "social_relation_output_implicit=rank_top_k(overall_group_item_output_mat, 100)\n",
        "\n",
        "\n",
        "Precision_at_k, Recall_at_k= calculate_Precision_Recall_at_K(social_relation_output_implicit, dataset.implicit_group_trainMatrix )\n",
        "\n",
        "#print(Precision_at_k)\n",
        "mean_Precision_at_k = sum(Precision_at_k) / len(Precision_at_k)\n",
        "print(\"Mean of Precision@k={}\".format(mean_Precision_at_k))\n",
        "\n",
        "print()\n",
        "#print(Recall_at_k)\n",
        "mean_Recall_at_k = sum(Recall_at_k) / len(Recall_at_k)\n",
        "print(\"Mean of Recall@k={}\".format(mean_Recall_at_k))\n",
        "\n",
        "##print(Precision_at_k)\n",
        "#mean_Pearson_Precision_at_k = sum(Precision_at_k_Pearson) / len(Precision_at_k_Pearson)\n",
        "#print(\"Mean of Pearson Precision@k={}\".format(mean_Pearson_Precision_at_k))\n",
        "#\n",
        "#print()\n",
        "##print(Recall_at_k)\n",
        "#mean_Pearson_Recall_at_k = sum(Recall_at_k_Pearson) / len(Recall_at_k_Pearson)\n",
        "#print(\"Mean of Pearson Recall@k={}\".format(mean_Pearson_Recall_at_k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ME_DB2zi6p"
      },
      "outputs": [],
      "source": [
        "Recall_at_k=[]\n",
        "Recall_at_k.append(Recall_at_k)\n",
        "#Recall_at_k.append(Recall_at_k_Jaccard)\n",
        "#Recall_at_k.append(Recall_at_k_Pearson)\n",
        "Recall_at_k_plot(Recall_at_k )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCIIqi9FyMvd"
      },
      "outputs": [],
      "source": [
        "Precision_at_k=[]\n",
        "Precision_at_k.append(Precision_at_k)\n",
        "#Precision_at_k.append(Precision_at_k_Jaccard)\n",
        "#Precision_at_k.append(Precision_at_k_Pearson)\n",
        "Precision_at_k_plot(Precision_at_k )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIVscj6geNPf"
      },
      "source": [
        "# *b. IMPLICIT RECOMMENDATION*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nPwEpo5eT_W"
      },
      "source": [
        "**Assumption:** \n",
        "\n",
        "1.   K=100\n",
        "\n",
        "\n",
        "\n",
        "\\begin{array}{|c|c|c|c|c|} \\hline\n",
        "Social Relationship & Expertise Descriptor & Dissimilarity Descriptor &Precision@K & Recall@K \\\\ \\hline\n",
        "yes & yes & Jaccard & 23.71\\% & 7.23\\%\\\\ \\hline\n",
        "yes & yes & Pearson & 23.71\\% & 7.23\\%\\\\ \\hline\n",
        "foo & bar & foo & bar & 1  \\\\ \\hline\n",
        "\\end{array}\n"
      ]
    }
  ]
}