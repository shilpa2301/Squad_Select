{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data available at : https://github.com/LianHaiMiao/Attentive-Group-Recommendation "
      ],
      "metadata": {
        "id": "1v63JXd1CdBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Exploration**"
      ],
      "metadata": {
        "id": "nO-9ps-FqUhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse as sp\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "geFhRZc5s3cw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "  def __init__(self):\n",
        "    self.data_path='/content/'"
      ],
      "metadata": {
        "id": "Jvb-o7MZsRlS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "r0H9hbAep_gj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Dataset(object):\n",
        "  def __init__ (self, data_path)  :\n",
        "        \n",
        "        print(\"loading User Train Matrix...\")\n",
        "        self.user_trainMatrix=self.load_rating_file_as_matrix(data_path+\"userRatingTrain.txt\")\n",
        "        print(\"loading Group Train Matrix...\")\n",
        "        self.group_trainMatrix=self.load_rating_file_as_matrix(data_path+\"groupRatingTrain.txt\")\n",
        "        print(\"loading User Test Matrix...\")\n",
        "        self.user_testMatrix=self.load_rating_file_as_matrix(data_path+\"userRatingTest.txt\")\n",
        "        print(\"loading Group Test Matrix...\")\n",
        "        self.group_testMatrix=self.load_rating_file_as_matrix(data_path+\"groupRatingTest.txt\")\n",
        "        \n",
        "        # process negative data\n",
        "        #print(\"loading User Negative into User test Matrix:\")\n",
        "        #self.user_testMatrix=self.load_negative_data(self.user_testMatrix, data_path+\"userRatingNegative.txt\")\n",
        "        \n",
        "        #taken num items as per train data since they are different in test matrices from train matrice item counts\n",
        "        self.num_users, self.num_items = self.user_trainMatrix.shape\n",
        "        self.num_groups=self.group_trainMatrix.shape[0]\n",
        "\n",
        "        #implicit matrices\n",
        "        self.implicit_user_trainMatrix=np.zeros((self.user_trainMatrix.shape))\n",
        "        self.implicit_user_testMatrix=np.zeros((self.user_testMatrix.shape))\n",
        "        self.implicit_group_trainMatrix=np.zeros((self.group_trainMatrix.shape))\n",
        "        self.implicit_group_testMatrix=np.zeros((self.group_testMatrix.shape))\n",
        "\n",
        "        print(\"loading Implicit User Train Matrix...\")\n",
        "        self.implicit_user_trainMatrix[self.user_trainMatrix!=0]=1\n",
        "        print(\"loading Implicit User Test Matrix...\")\n",
        "        self.implicit_user_testMatrix[self.user_testMatrix!=0]=1\n",
        "        print(\"loading Implicit Group Train Matrix...\")\n",
        "        self.implicit_group_trainMatrix[self.group_trainMatrix!=0]=1\n",
        "        print(\"loading Implicit Group Test Matrix...\")\n",
        "        self.implicit_group_testMatrix[self.group_testMatrix!=0]=1\n",
        "\n",
        "        #group-user mapping\n",
        "        print(\"loading Group-User Mapping Data...\")\n",
        "        self.group_user_Dict=self.extract_group_user_data(data_path+\"groupMember.txt\")\n",
        "\n",
        "  def extract_group_user_data(self, filename):\n",
        "        group_user_dict={}\n",
        "        with open(filename, \"r\") as f:\n",
        "            line=f.readline()\n",
        "            while line!=None and line!=\"\":\n",
        "                arr=line.split(\" \")\n",
        "                arr[1] = arr[1].replace(\"\\n\", \"\")\n",
        "                members=arr[1].split(\",\")\n",
        "                if arr[0] not in group_user_dict:\n",
        "                    group_user_dict[int(arr[0])]= [int(x) for x in members]\n",
        "                line=f.readline()\n",
        "        return group_user_dict\n",
        "\n",
        "  def load_rating_file_as_matrix(self, filename):\n",
        "        # Get number of users and items\n",
        "        num_users, num_items = 0, 0\n",
        "        with open(filename, \"r\") as f:\n",
        "            line=f.readline()\n",
        "            while line!=None and line!=\"\":\n",
        "                arr=line.split(\" \")\n",
        "                u,i=int(arr[0]), int(arr[1])\n",
        "                num_users=max(num_users,u)\n",
        "                num_items=max(num_items,i)\n",
        "                line=f.readline()\n",
        "      \n",
        "        mat=np.zeros((num_users+1, num_items+1))\n",
        "        with open(filename, \"r\") as f:\n",
        "            line=f.readline()\n",
        "            while line!=None and line!=\"\":\n",
        "                arr=line.split(\" \")\n",
        "                mat[int(arr[0])-1][int(arr[1])-1]=int(arr[2])\n",
        "                line=f.readline()\n",
        "\n",
        "        return mat\n",
        "\n",
        "  #TBD\n",
        "  def load_negative_data(self, matrixname, filename):\n",
        "        with open(filename, \"r\") as f:\n",
        "            line=f.readline()\n",
        "            while line!=None and line!=\"\":\n",
        "                arr=line.split(\" \")\n",
        "                print(arr)\n",
        "                line=f.readline()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  config=Config()\n",
        "  dataset=Dataset(config.data_path)\n",
        "  num_users, num_items, num_groups = dataset.num_users, dataset.num_items, dataset.num_groups\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzk5-DDxstV6",
        "outputId": "fb028c54-6643-4a59-8af2-f23e1d5e1835"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading User Train Matrix...\n",
            "loading Group Train Matrix...\n",
            "loading User Test Matrix...\n",
            "loading Group Test Matrix...\n",
            "loading Implicit User Train Matrix...\n",
            "loading Implicit User Test Matrix...\n",
            "loading Implicit Group Train Matrix...\n",
            "loading Implicit Group Test Matrix...\n",
            "loading Group-User Mapping Data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Details:**"
      ],
      "metadata": {
        "id": "uy7Bldhn7G4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"num users, num items, num groups=({}, {}, {})\".format(num_users, num_items, num_groups))\n",
        "print()\n",
        "print(\"user_traindata shape={}\".format(dataset.user_trainMatrix.shape))\n",
        "print(\"user_testdata shape={}\".format(dataset.user_testMatrix.shape))\n",
        "print(\"group_traindata shape={}\".format(dataset.group_trainMatrix.shape))\n",
        "print(\"group_testdata shape={}\".format(dataset.group_testMatrix.shape))\n",
        "print()\n",
        "print(\"Implicit user_traindata shape={}\".format( dataset.implicit_user_trainMatrix.shape))\n",
        "print(\"implicit user_testdata shape={}\".format(  dataset.implicit_user_testMatrix.shape))\n",
        "print(\"implicit group_traindata shape={}\".format(dataset.implicit_group_trainMatrix.shape))\n",
        "print(\"implicit group_testdata shape={}\".format( dataset.implicit_group_testMatrix.shape))\n",
        "print()\n",
        "print('Group User Data:')\n",
        "print(dataset.group_user_Dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cp1MRnbt31Y",
        "outputId": "03f3ccbc-e7bc-46d3-fe9a-f26ef02f4101"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num users, num items, num groups=(602, 7710, 290)\n",
            "\n",
            "user_traindata shape=(602, 7710)\n",
            "user_testdata shape=(602, 7679)\n",
            "group_traindata shape=(290, 7710)\n",
            "group_testdata shape=(290, 7656)\n",
            "\n",
            "Implicit user_traindata shape=(602, 7710)\n",
            "implicit user_testdata shape=(602, 7679)\n",
            "implicit group_traindata shape=(290, 7710)\n",
            "implicit group_testdata shape=(290, 7656)\n",
            "\n",
            "Group User Data:\n",
            "{216: [346, 414], 217: [433, 526], 214: [559, 570], 215: [226, 294], 212: [415, 470], 213: [43, 267, 308], 210: [443, 520], 211: [53, 392], 165: [451, 496], 264: [105, 171], 265: [556, 253, 366], 218: [334, 386], 219: [199, 302], 133: [6, 126], 132: [141, 519], 131: [480, 500], 130: [179, 348], 137: [106, 524], 136: [304, 587], 135: [42, 510], 134: [113, 120], 139: [440, 545], 138: [365, 490], 166: [258, 397], 24: [27, 404], 25: [58, 252], 26: [157, 565, 431], 27: [347, 462], 20: [8, 435], 21: [152, 484], 22: [271, 502], 23: [155, 381], 160: [391, 405], 28: [597, 521], 29: [23, 523], 161: [210, 486], 289: [61, 475], 0: [21, 198], 4: [83, 136], 281: [135, 151], 8: [93, 99], 283: [183, 352], 163: [575, 530], 285: [139, 268], 284: [70, 413], 287: [19, 167, 282, 283], 286: [2, 31], 119: [311, 343], 258: [122, 315], 120: [395, 551], 121: [9, 46], 122: [116, 329], 123: [33, 400], 124: [222, 428], 125: [172, 204], 126: [0, 485], 127: [129, 376], 128: [340, 396], 129: [213, 291], 269: [110, 364], 268: [234, 529], 167: [188, 208], 118: [508, 515], 59: [153, 235], 58: [319, 419], 55: [287, 382], 54: [303, 374], 57: [248, 305], 56: [114, 349], 51: [239, 344], 50: [375, 507], 53: [378, 420], 52: [127, 261], 259: [11, 416], 276: [181, 505], 164: [68, 534], 201: [60, 353, 368], 199: [76, 549], 179: [112, 552], 200: [148, 367], 195: [47, 401], 194: [249, 472], 197: [160, 567, 424, 491], 178: [77, 498], 191: [85, 447], 190: [566, 278], 193: [236, 425], 192: [88, 460], 115: [323, 458], 114: [557, 482], 88: [12, 214], 89: [173, 438], 111: [80, 193], 110: [298, 383], 113: [421, 481], 112: [15, 202], 82: [180, 328], 83: [102, 409], 80: [51, 262], 81: [36, 336], 86: [57, 270], 87: [158, 423], 84: [300, 449], 85: [238, 467], 251: [108, 471], 198: [560, 562], 256: [418, 493], 206: [483, 522], 226: [159, 377], 257: [339, 459], 3: [86, 189], 177: [45, 192], 254: [292, 544], 7: [216, 599], 247: [81, 406], 273: [73, 430], 255: [555, 264], 225: [104, 140], 245: [133, 290], 244: [49, 539], 108: [72, 233], 109: [34, 169], 241: [547, 553], 240: [14, 511], 243: [332, 437], 242: [465, 533], 102: [186, 469], 103: [50, 537], 100: [257, 578], 101: [247, 359], 106: [65, 109], 107: [190, 422], 104: [588, 506], 105: [432, 590], 39: [144, 488], 38: [22, 306], 33: [111, 429], 32: [7, 243], 31: [97, 98, 166], 30: [220, 309], 37: [245, 456], 36: [231, 307], 35: [66, 250], 34: [170, 412], 246: [285, 504], 282: [145, 543], 252: [317, 322], 205: [579, 466], 223: [569, 581], 176: [572, 513], 60: [360, 441], 61: [128, 299], 62: [32, 436], 63: [227, 501], 64: [358, 442], 65: [312, 342], 66: [82, 95], 67: [399, 525], 68: [276, 546], 69: [3, 205, 297], 175: [30, 379], 174: [79, 265], 173: [574, 351], 172: [568, 284], 171: [219, 246], 170: [476, 516], 203: [69, 474], 222: [212, 363], 288: [156, 274], 181: [149, 454], 253: [78, 228], 248: [331, 371], 182: [174, 197], 183: [580, 583], 180: [337, 494], 2: [24, 187], 162: [71, 563], 187: [178, 333], 184: [35, 558], 6: [154, 177], 220: [357, 582], 186: [26, 503], 188: [206, 591], 189: [90, 313, 325], 202: [200, 301], 196: [25, 561, 411], 221: [223, 384], 185: [237, 446], 271: [29, 91], 99: [131, 335], 98: [254, 457], 168: [495, 509], 169: [55, 402], 229: [52, 194], 228: [370, 448], 91: [92, 259], 90: [28, 241], 93: [380, 389], 92: [314, 463], 95: [20, 101], 94: [240, 408], 97: [464, 473], 96: [211, 350], 11: [143, 182], 10: [115, 518], 13: [39, 354, 453], 12: [295, 499], 15: [217, 280], 14: [100, 373], 17: [576, 417], 16: [330, 531], 19: [41, 142], 18: [125, 541], 117: [256, 492], 116: [75, 293], 270: [146, 517], 274: [54, 341, 589], 204: [209, 594], 224: [564, 191], 275: [601, 586], 151: [130, 277], 150: [74, 478], 153: [165, 479], 152: [279, 445], 155: [40, 162], 154: [107, 361], 157: [118, 403], 156: [318, 372], 159: [119, 123], 158: [310, 452, 540], 277: [224, 514], 234: [320, 487], 238: [134, 455], 239: [362, 585], 227: [577, 345], 279: [96, 461], 207: [201, 571], 235: [48, 215], 236: [439, 584], 237: [1, 260], 230: [573, 321], 231: [4, 132, 444, 593], 232: [497, 527], 233: [13, 369], 280: [161, 164, 185], 48: [117, 218], 49: [16, 17], 46: [67, 326], 47: [168, 288], 44: [269, 398], 45: [221, 356], 42: [281, 286], 43: [554, 230], 40: [385, 410], 41: [316, 450], 1: [427, 548], 5: [5, 489], 9: [273, 394], 272: [44, 64, 87, 324], 146: [38, 84], 147: [592, 542], 144: [595, 251], 145: [176, 598, 528], 142: [94, 338], 143: [18, 147], 140: [175, 272], 141: [138, 150], 209: [242, 407], 208: [266, 387], 148: [63, 89], 149: [103, 550], 77: [512, 532], 76: [196, 296], 75: [225, 255], 74: [62, 289], 73: [203, 538], 72: [184, 434], 71: [56, 275], 70: [37, 327], 278: [121, 263, 390], 79: [195, 393], 78: [163, 229], 263: [244, 536], 249: [137, 426], 262: [207, 468], 261: [232, 355], 250: [124, 388], 260: [600, 596], 267: [59, 535], 266: [10, 477]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: Need to check :**\n",
        "\n",
        "\n",
        "\n",
        "1.   Why is test data's item counts different from train data's\n",
        "2.   Plan how to incorporate and make use of negative data wrt to each item and each user/group\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KBl1VJdZDIPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Module Function Scripts**"
      ],
      "metadata": {
        "id": "1MUCmLUkCv7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gartrell, M., Xing, X., Lv, Q., Beach, A., Han, R., Mishra, S., & Seada, K. (2010, November). Enhancing group recommendation by incorporating social relationship interactions. In Proceedings of the 16th ACM international conference on Supporting group work (pp. 97-106).**"
      ],
      "metadata": {
        "id": "wTz-Y_X6QwIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. Social Relationship***"
      ],
      "metadata": {
        "id": "uIcPDG3JQXmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_in_group=np.zeros(num_groups)\n",
        "\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  sum_of_weight_similarities=0\n",
        "  \n",
        "  \n",
        "  for i in range(len(group_members)):\n",
        "    for j in range(i+1,len(group_members)):\n",
        "      user1=group_members[i]\n",
        "      user2=group_members[j]\n",
        "      if user1 != user2:\n",
        "        w_ij=0\n",
        "        user1_items=np.nonzero(dataset.user_trainMatrix[user1])[0]\n",
        "        user2_items=np.nonzero(dataset.user_trainMatrix[user2])[0]\n",
        "        common_items = set(user1_items).intersection(user2_items)\n",
        "        num_common_items = len(common_items)\n",
        "        total_items = set(user1_items).union(user2_items)\n",
        "        num_total_items = len(total_items)\n",
        "\n",
        "        w_ij=num_common_items/num_total_items\n",
        "        sum_of_weight_similarities+=w_ij\n",
        "  similarity_in_group[group_id]=(2*sum_of_weight_similarities)/(len(group_members)*(len(group_members)-1))\n",
        "\n",
        "#print(similarity_in_group)   "
      ],
      "metadata": {
        "id": "xSXj6tSkPu9l"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "social_descriptor_groupwise=np.zeros(num_groups)\n",
        "sorted_values = sorted(similarity_in_group)\n",
        "\n",
        "# Compute the statistical thresholds for each category (since we dont have very strong similarities in any group)\n",
        "q1 = sorted_values[int(len(sorted_values) * 0.33)]\n",
        "q2 = sorted_values[int(len(sorted_values) * 0.67)]\n",
        "\n",
        "\n",
        "#print(q1,\",\",q2)\n",
        "\n",
        "for i in range(num_groups):\n",
        "        if similarity_in_group[i] <= q1:\n",
        "            social_descriptor_groupwise[i]=0\n",
        "        elif similarity_in_group[i] <= q2:\n",
        "            social_descriptor_groupwise[i]=1\n",
        "        else:\n",
        "            social_descriptor_groupwise[i]=2\n",
        "    \n",
        "#print(social_descriptor_groupwise)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EQXJT5QGpINE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2. Expertise descriptor***"
      ],
      "metadata": {
        "id": "QBt8QboyQanG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate absolute user expertise\n",
        "user_expertise=np.zeros(num_users)\n",
        "x=0\n",
        "for user_data in dataset.user_trainMatrix:\n",
        "    #print(user_data)\n",
        "    user_i_item_count=len(np.nonzero(user_data)[0])\n",
        "    #print(user_i_item_count)\n",
        "    user_expertise[x]=user_i_item_count/num_items\n",
        "    x+=1\n",
        "\n",
        "#print(user_expertise)\n",
        "absolute_user_expertise_levels=np.zeros(num_users)\n",
        "sorted_expertise_values = sorted(user_expertise)\n",
        "\n",
        "# Compute the statistical thresholds for each category (since we dont have very strong similarities in any group)\n",
        "q1 = sorted_expertise_values[int(len(sorted_expertise_values) * 0.2)]\n",
        "q2 = sorted_expertise_values[int(len(sorted_expertise_values) * 0.4)]\n",
        "q3 = sorted_expertise_values[int(len(sorted_expertise_values) * 0.6)]\n",
        "q4 = sorted_expertise_values[int(len(sorted_expertise_values) * 0.8)]\n",
        "\n",
        "#print(q1,\",\",q2,\",\",q3,\",\",q4)\n",
        "\n",
        "for i in range(num_users):\n",
        "        if user_expertise[i] <= q1:\n",
        "            absolute_user_expertise_levels[i]=1\n",
        "        elif user_expertise[i] <= q2:\n",
        "            absolute_user_expertise_levels[i]=2\n",
        "        elif user_expertise[i] <= q3:\n",
        "            absolute_user_expertise_levels[i]=3\n",
        "        elif user_expertise[i] <= q4:\n",
        "            absolute_user_expertise_levels[i]=4\n",
        "        else:\n",
        "            absolute_user_expertise_levels[i]=5\n",
        "    \n",
        "#print(absolute_user_expertise_levels)\n"
      ],
      "metadata": {
        "id": "0_adRkknPUSx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#uncomment this line for 2d sparse matrix\n",
        "expertise_in_group_members_mat=np.zeros((num_groups, num_users))\n",
        "#uncomment this line for dictionary of list(of dicts)\n",
        "#expertise_in_group_members_dict={}\n",
        "\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  \n",
        "  sum_of_expertise_in_group=0 \n",
        "  for user in group_members:\n",
        "      sum_of_expertise_in_group+=absolute_user_expertise_levels[user]\n",
        "  list_of_member_expertise_groupwise=[] \n",
        "  for user in group_members:\n",
        "      user_relative_expertise_pair={}\n",
        "      E_j=absolute_user_expertise_levels[user]/sum_of_expertise_in_group\n",
        "      user_relative_expertise_pair[user]=E_j\n",
        "\n",
        "      #uncomment this line for 2d sparse matrix\n",
        "      expertise_in_group_members_mat[group_id][user]=E_j\n",
        "      #uncomment this two lines for dictionary of list(of dicts)\n",
        "      #list_of_member_expertise_groupwise.append(user_relative_expertise_pair)\n",
        "  #expertise_in_group_members_dict[group_id]=list_of_member_expertise_groupwise\n",
        "\n",
        "#print(expertise_in_group_members_mat[1])\n"
      ],
      "metadata": {
        "id": "ri9uefIGTP46"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3. Dissimilarity descriptors***"
      ],
      "metadata": {
        "id": "ba_qjLEvQdAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Explicit Rating- taking rating of items into consideration**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H5FA152lXWjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Implicit Rating**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pJX_5E7fXsN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Dis_1: APD(Average Pairwise Dissimilarity)"
      ],
      "metadata": {
        "id": "FjrCCz49Wjfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#explicit APD\n",
        "\n",
        "APD_mat=np.zeros((num_groups, num_items))\n",
        "\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "  for item in range(num_items):\n",
        "      sum_rating_deviation=0\n",
        "      sum_rating_deviation = sum(abs(dataset.user_trainMatrix[i][item]-dataset.user_trainMatrix[j][item]) for i in range(num_members) for j in range(i+1, num_members))\n",
        "\n",
        "      \n",
        "      APD_mat[group_id][item]=(2*sum_rating_deviation)/(num_members*(num_members-1))\n",
        "\n",
        "#print(APD_mat)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rBFh_xxtXKVN"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implicit APD\n",
        "\n",
        "APD_mat_implicit=np.zeros((num_groups, num_items))\n",
        "\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "  for item in range(num_items):\n",
        "      sum_rating_deviation=0\n",
        "      sum_rating_deviation = sum(abs(dataset.implicit_user_trainMatrix[i][item]-dataset.implicit_user_trainMatrix[j][item]) for i in range(num_members) for j in range(i+1, num_members))\n",
        "\n",
        "      \n",
        "      APD_mat_implicit[group_id][item]=(2*sum_rating_deviation)/(num_members*(num_members-1))\n",
        "\n",
        "#print(APD_mat_implicit)"
      ],
      "metadata": {
        "id": "ct_3v4Ag0aSq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Dis_2: VD(Variance Dissimilarity)"
      ],
      "metadata": {
        "id": "ISI8MRHwWv_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#explicit VD\n",
        "\n",
        "VD_mat=np.zeros((num_groups, num_items))\n",
        "\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "  \n",
        "\n",
        "  for i in range(num_items):\n",
        "      avg_for_item_groupwise = sum(dataset.user_trainMatrix[group_members[m]][i] for m in range(num_members)) / (num_members)\n",
        "      sum_rating_deviation_from_avg=0\n",
        "      for j in range(num_members):\n",
        "            sum_rating_deviation_from_avg+= (dataset.user_trainMatrix[group_members[j]][i]-avg_for_item_groupwise)**2\n",
        "      VD_mat[group_id][i]=sum_rating_deviation_from_avg/num_members\n",
        "\n",
        "#print(VD_mat)"
      ],
      "metadata": {
        "id": "4nOWPxgcuEZL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implicit VD\n",
        "\n",
        "VD_mat_implicit=np.zeros((num_groups, num_items))\n",
        "\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "  \n",
        "\n",
        "  for i in range(num_items):\n",
        "      avg_for_item_groupwise = sum(dataset.implicit_user_trainMatrix[group_members[m]][i] for m in range(num_members)) / (num_members)\n",
        "      sum_rating_deviation_from_avg=0\n",
        "      for j in range(num_members):\n",
        "            sum_rating_deviation_from_avg+= (dataset.implicit_user_trainMatrix[group_members[j]][i]-avg_for_item_groupwise)**2\n",
        "      VD_mat_implicit[group_id][i]=sum_rating_deviation_from_avg/num_members\n",
        "\n",
        "#print(VD_mat_implicit)"
      ],
      "metadata": {
        "id": "3B3myQ_0044Y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4. Heuristic group concensus function***"
      ],
      "metadata": {
        "id": "kex61McsQgEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicit rating**"
      ],
      "metadata": {
        "id": "ppmmybMq1YUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using Dis_1= APD\n",
        "w_1=0.8\n",
        "w_2=0.2\n",
        "\n",
        "overall_group_item_output_mat_APD=np.zeros((num_groups, num_items))\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "\n",
        "  #Ei_Rix=np.zeros((num_members,num_items))\n",
        "  #x=0\n",
        "  #for member in group_members:\n",
        "  #    for item in range(num_items):\n",
        "  #        Ei_Rix[x][item]=expertise_in_group_members_mat[group_id][member]*dataset.user_trainMatrix[member][item]\n",
        "  #    x+=1\n",
        "#\n",
        "  Ei_Rix = np.array([[expertise_in_group_members_mat[group_id][member] * dataset.user_trainMatrix[member][item] for item in range(num_items)] for member in group_members])\n",
        "\n",
        "\n",
        "  for item in range(num_items): \n",
        "      if social_descriptor_groupwise[group_id]==0:\n",
        "          overall_group_item_output_mat_APD[group_id][item]= w_1*min(Ei_Rix[:, item].tolist())+w_2*APD_mat[group_id][item] #min_misery\n",
        "      elif social_descriptor_groupwise[group_id]==1:\n",
        "          overall_group_item_output_mat_APD[group_id][item]= w_1*(sum(Ei_Rix[:, item].tolist()) / len(Ei_Rix[:, item].tolist()))+w_2*APD_mat[group_id][item] #avg_satisfaction\n",
        "      elif social_descriptor_groupwise[group_id]==2:\n",
        "          overall_group_item_output_mat_APD[group_id][item]= w_1*max(Ei_Rix[:, item].tolist())+w_2*APD_mat[group_id][item] #max_satisfaction\n",
        "\n",
        "\n",
        "print(overall_group_item_output_mat_APD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d050ze391XK3",
        "outputId": "5b30a737-5bdf-4010-d623-c426c642edbf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.          0.         10.         ...  0.          0.\n",
            "  14.        ]\n",
            " [ 0.          0.         10.         ...  0.          0.\n",
            "  14.        ]\n",
            " [ 0.          0.         10.         ...  0.          0.\n",
            "  14.        ]\n",
            " ...\n",
            " [ 9.          0.          9.16666667 ...  0.          0.\n",
            "  29.33333333]\n",
            " [ 0.          0.         30.         ...  0.          0.\n",
            "  31.33333333]\n",
            " [ 0.          0.         10.         ...  0.          0.\n",
            "  31.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using Dis_2= VD\n",
        "w_1=0.8\n",
        "w_2=0.2\n",
        "\n",
        "overall_group_item_output_mat_VD=np.zeros((num_groups, num_items))\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "\n",
        "  #Ei_Rix=np.zeros((num_members,num_items))\n",
        "  #x=0\n",
        "  #for member in group_members:\n",
        "  #    for item in range(num_items):\n",
        "  #        Ei_Rix[x][item]=expertise_in_group_members_mat[group_id][member]*dataset.user_trainMatrix[member][item]\n",
        "  #    x+=1\n",
        "#\n",
        "  Ei_Rix = np.array([[expertise_in_group_members_mat[group_id][member] * dataset.user_trainMatrix[member][item] for item in range(num_items)] for member in group_members])\n",
        "\n",
        "\n",
        "  for item in range(num_items): \n",
        "      if social_descriptor_groupwise[group_id]==0:\n",
        "          overall_group_item_output_mat_VD[group_id][item]= w_1*min(Ei_Rix[:, item].tolist())+w_2*VD_mat[group_id][item] #min_misery\n",
        "      elif social_descriptor_groupwise[group_id]==1:\n",
        "          overall_group_item_output_mat_VD[group_id][item]= w_1*(sum(Ei_Rix[:, item].tolist()) / len(Ei_Rix[:, item].tolist()))+w_2*VD_mat[group_id][item] #avg_satisfaction\n",
        "      elif social_descriptor_groupwise[group_id]==2:\n",
        "          overall_group_item_output_mat_VD[group_id][item]= w_1*max(Ei_Rix[:, item].tolist())+w_2*VD_mat[group_id][item] #max_satisfaction\n",
        "\n",
        "\n",
        "print(overall_group_item_output_mat_VD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyg9NlJJ-qti",
        "outputId": "81672a4e-ee11-4b3c-8bed-5e1cd30eb755"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0.           0.           0.         ...   0.           0.\n",
            "  211.25      ]\n",
            " [405.           0.           0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [  0.           0.           0.         ...   0.           0.\n",
            "    0.        ]\n",
            " ...\n",
            " [  0.           0.           0.         ...   0.           0.\n",
            "  253.75      ]\n",
            " [  0.           0.         301.25       ...   0.           0.\n",
            "  228.58333333]\n",
            " [  0.           0.           0.         ...   0.           0.\n",
            "  262.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implicit rating**"
      ],
      "metadata": {
        "id": "NbJ1EiLD1hx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using Dis_1= APD implicit\n",
        "w_1=0.8\n",
        "w_2=0.2\n",
        "\n",
        "overall_group_item_output_mat_APD_implicit=np.zeros((num_groups, num_items))\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "\n",
        "  #Ei_Rix=np.zeros((num_members,num_items))\n",
        "  #x=0\n",
        "  #for member in group_members:\n",
        "  #    for item in range(num_items):\n",
        "  #        Ei_Rix[x][item]=expertise_in_group_members_mat[group_id][member]*dataset.user_trainMatrix[member][item]\n",
        "  #    x+=1\n",
        "#\n",
        "  Ei_Rix = np.array([[expertise_in_group_members_mat[group_id][member] * dataset.implicit_user_trainMatrix[member][item] for item in range(num_items)] for member in group_members])\n",
        "\n",
        "\n",
        "  for item in range(num_items): \n",
        "      if social_descriptor_groupwise[group_id]==0:\n",
        "          overall_group_item_output_mat_APD_implicit[group_id][item]= w_1*min(Ei_Rix[:, item].tolist())+w_2*APD_mat_implicit[group_id][item] #min_misery\n",
        "      elif social_descriptor_groupwise[group_id]==1:\n",
        "          overall_group_item_output_mat_APD_implicit[group_id][item]= w_1*(sum(Ei_Rix[:, item].tolist()) / len(Ei_Rix[:, item].tolist()))+w_2*APD_mat_implicit[group_id][item] #avg_satisfaction\n",
        "      elif social_descriptor_groupwise[group_id]==2:\n",
        "          overall_group_item_output_mat_APD_implicit[group_id][item]= w_1*max(Ei_Rix[:, item].tolist())+w_2*APD_mat_implicit[group_id][item] #max_satisfaction\n",
        "\n",
        "\n",
        "print(overall_group_item_output_mat_APD_implicit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of2VgmJJ1hRj",
        "outputId": "f4e56764-b430-4fe4-900a-175534259104"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.2        ... 0.         0.         0.2       ]\n",
            " [0.         0.         0.2        ... 0.         0.         0.2       ]\n",
            " [0.         0.         0.2        ... 0.         0.         0.2       ]\n",
            " ...\n",
            " [0.1        0.         0.13333333 ... 0.         0.         0.38333333]\n",
            " [0.         0.         0.46666667 ... 0.         0.         0.46666667]\n",
            " [0.         0.         0.2        ... 0.         0.         0.45      ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using Dis_2= VD implicit\n",
        "w_1=0.8\n",
        "w_2=0.2\n",
        "\n",
        "overall_group_item_output_mat_VD_implicit=np.zeros((num_groups, num_items))\n",
        "for group_id in dataset.group_user_Dict:\n",
        "  group_members= dataset.group_user_Dict[group_id]\n",
        "  num_members=len(group_members)\n",
        "\n",
        "  #Ei_Rix=np.zeros((num_members,num_items))\n",
        "  #x=0\n",
        "  #for member in group_members:\n",
        "  #    for item in range(num_items):\n",
        "  #        Ei_Rix[x][item]=expertise_in_group_members_mat[group_id][member]*dataset.user_trainMatrix[member][item]\n",
        "  #    x+=1\n",
        "#\n",
        "  Ei_Rix = np.array([[expertise_in_group_members_mat[group_id][member] * dataset.implicit_user_trainMatrix[member][item] for item in range(num_items)] for member in group_members])\n",
        "\n",
        "\n",
        "  for item in range(num_items): \n",
        "      if social_descriptor_groupwise[group_id]==0:\n",
        "          overall_group_item_output_mat_VD_implicit[group_id][item]= w_1*min(Ei_Rix[:, item].tolist())+w_2*VD_mat_implicit[group_id][item] #min_misery\n",
        "      elif social_descriptor_groupwise[group_id]==1:\n",
        "          overall_group_item_output_mat_VD_implicit[group_id][item]= w_1*(sum(Ei_Rix[:, item].tolist()) / len(Ei_Rix[:, item].tolist()))+w_2*VD_mat_implicit[group_id][item] #avg_satisfaction\n",
        "      elif social_descriptor_groupwise[group_id]==2:\n",
        "          overall_group_item_output_mat_VD_implicit[group_id][item]= w_1*max(Ei_Rix[:, item].tolist())+w_2*VD_mat_implicit[group_id][item] #max_satisfaction\n",
        "\n",
        "\n",
        "print(overall_group_item_output_mat_VD_implicit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAtwWA-o_iAc",
        "outputId": "8bbcc67e-7b96-41bf-c770-6dd97a05dc29"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.05      ]\n",
            " [0.05       0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.3       ]\n",
            " [0.         0.         0.31666667 ... 0.         0.         0.31666667]\n",
            " [0.         0.         0.         ... 0.         0.         0.3       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rank top k**"
      ],
      "metadata": {
        "id": "ooWbulNeQjfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_top_k(group_item_mat, k):\n",
        "    ranked_top_k=[]\n",
        "    for group_data in group_item_mat:\n",
        "        sorted_top_k_groupwise=sorted(range(len(group_data)), key=lambda i: group_data[i], reverse=True)[:k]\n",
        "        ranked_top_k.append(sorted_top_k_groupwise)\n",
        "\n",
        "    return ranked_top_k"
      ],
      "metadata": {
        "id": "wBJ-Ir6SAEP2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "social_relation_output_APD_explicit=rank_top_k(overall_group_item_output_mat_APD, 100)\n",
        "social_relation_output_VD_explicit=rank_top_k(overall_group_item_output_mat_VD, 100)\n",
        "social_relation_output_APD_implicit=rank_top_k(overall_group_item_output_mat_APD_implicit, 100)\n",
        "social_relation_output_VD_implicit=rank_top_k(overall_group_item_output_mat_VD_implicit, 100)"
      ],
      "metadata": {
        "id": "pHUzZc_OBS0I"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Permuted Pipeline creation**"
      ],
      "metadata": {
        "id": "OrYXaHcRC1hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DISCUSS: How do we create?**\n",
        "1. Do we generate top 1000 from each algo and take intersection and then output top 50?\n",
        "2. Do we create top 1000 from first algorithm, reduce no of columns of entire dataset, perform second algorithm to output 900 items, then reduce trainset to those common items and so on.. in the end take top 50?"
      ],
      "metadata": {
        "id": "7YhxIihNr9XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "8LEi50QnC7-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DISCUSS:**\n",
        "\n",
        "1. Should we use group train data as validation set and predict new recommendations for test data?\n",
        "2. Should we add some weight to training data and perform:\n",
        "  new recommendation= (W * group_train_data) + ((1-W)* generated recommendation from individual data)"
      ],
      "metadata": {
        "id": "YK2F2bYGCTBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis**"
      ],
      "metadata": {
        "id": "h_ADskKkC_OB"
      }
    }
  ]
}